{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dpath='NL2SparQL4NLU/dataset/NL2SparQL4NLU'\n",
    "spath='dataset'\n",
    "mkdir -p $spath\n",
    "\n",
    "cp $dpath.train.utterances.txt $spath/trn.txt\n",
    "cp $dpath.test.utterances.txt $spath/tst.txt\n",
    "\n",
    "cp $dpath.train.conll.txt $spath/trn.conll\n",
    "cp $dpath.test.conll.txt $spath/tst.conll\n",
    "\n",
    "cp $dpath.train.features.conll.txt $spath/trn.feature.conll\n",
    "cp $dpath.test.features.conll.txt $spath/tst.feature.conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Concept Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.conll import ConllChunkCorpusReader\n",
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "from nltk.tag import NgramTagger \n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(corpus_file, fs=\"\\t\"):\n",
    "    sents = read_corpus_conll(corpus_file, fs=fs)\n",
    "    return set([token[-1] for sent in sents for token in sent if token[-1] != 'O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOB-Tags HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.867\n",
      "F1 score: 77.470\n"
     ]
    }
   ],
   "source": [
    "trn='dataset/trn.conll'\n",
    "\n",
    "concepts = sorted(get_chunks(trn))\n",
    "\n",
    "trn_data = ConllChunkCorpusReader('dataset/',  r'trn.conll', concepts)\n",
    "tst_data = ConllChunkCorpusReader('dataset/',  r'tst.conll', concepts)\n",
    "\n",
    "# training hmm on training data\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = hmm_model.train(trn_data.tagged_sents())\n",
    "\n",
    "# getting references\n",
    "refs = [str(t[1][2:]) for s in tst_data.tagged_sents() for t in s]\n",
    "\n",
    "# getting hypotheses\n",
    "hyps = [str(t[1][2:]) for s in tst_data.sents() for t in hmm_tagger.tag(s)]\n",
    "\n",
    "# print scores\n",
    "tags = [c[2:] for c in concepts]\n",
    "\n",
    "accuracy = hmm_tagger.evaluate(tst_data.tagged_sents())\n",
    "print(\"Accuracy: {:6.3f}\".format(accuracy*100))\n",
    "\n",
    "f1 = metrics.f1_score(refs, hyps, average=\"weighted\", labels=np.unique(tags), zero_division=0)\n",
    "print(\"F1 score: {:6.3f}\".format(f1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOB-Tags NgramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_tagger(tagged_sents, tagger_classes, backoff=None):\n",
    "    if not backoff:\n",
    "        backoff = tagger_classes[0](tagged_sents)\n",
    "        del tagger_classes[0]\n",
    " \n",
    "    for cls in tagger_classes:\n",
    "        tagger = cls(tagged_sents, backoff=backoff)\n",
    "        backoff = tagger\n",
    " \n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff: 0\tNgram: 1\tAcc:  87.62\tF1 :  74.03\n",
      "Cutoff: 0\tNgram: 2\tAcc:  77.98\tF1 :  70.07\n",
      "Cutoff: 0\tNgram: 3\tAcc:  75.48\tF1 :  68.06\n",
      "\n",
      "\n",
      "Cutoff: 1\tNgram: 1\tAcc:  85.72\tF1 :  71.44\n",
      "Cutoff: 1\tNgram: 2\tAcc:  71.59\tF1 :  62.91\n",
      "Cutoff: 1\tNgram: 3\tAcc:  68.84\tF1 :  60.47\n",
      "\n",
      "\n",
      "\t\tCombined:\tAcc:  90.97\tF1 :  78.76\n"
     ]
    }
   ],
   "source": [
    "ngram = [1,2,3]\n",
    "cutoff = [0,1]\n",
    "for co in cutoff:\n",
    "    for ngo in ngram:\n",
    "        nm = NgramTagger(ngo, trn_data.tagged_sents(), cutoff=co)\n",
    "        acc = nm.evaluate(tst_data.tagged_sents())\n",
    "        \n",
    "        refs = [str(t[1][2:]) for s in tst_data.tagged_sents() for t in s] \n",
    "        hyps = [str(t[1][2:]) if t[1] is not None else str(t[1]) for s in tst_data.sents() for t in nm.tag(s)]\n",
    "        \n",
    "        f1 = metrics.f1_score(refs, hyps, average=\"weighted\", labels=np.unique(tags), zero_division=0)\n",
    "        \n",
    "        print(\"Cutoff: {}\\tNgram: {}\\tAcc: {:6.2f}\\tF1 : {:6.2f}\".format(co,ngo,acc*100,f1*100))  \n",
    "    print(\"\\n\")\n",
    "\n",
    "backoff = DefaultTagger('O')\n",
    "tag = backoff_tagger(trn_data.tagged_sents(),[UnigramTagger, BigramTagger, TrigramTagger],backoff=backoff)\n",
    "acc = tag.evaluate(tst_data.tagged_sents())\n",
    "refs = [str(t[1][2:]) for s in tst_data.tagged_sents() for t in s]\n",
    "hyps = [str(t[1][2:]) if t[1] is not None else str(t[1]) for s in tst_data.sents() for t in tag.tag(s)]\n",
    "f1 = metrics.f1_score(refs, hyps, average=\"weighted\", labels=np.unique(tags), zero_division=0)\n",
    "\n",
    "print(\"\\t\\tCombined:\\tAcc: {:6.2f}\\tF1 : {:6.2f}\".format(acc*100,f1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-Tags HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.881\n",
      "F1 score: 87.379\n"
     ]
    }
   ],
   "source": [
    "trn='dataset/trn.feature.conll'\n",
    "\n",
    "concepts = sorted(get_chunks(trn))\n",
    "\n",
    "trn_data = ConllChunkCorpusReader('dataset/',  r'trn.feature.conll', concepts)\n",
    "tst_data = ConllChunkCorpusReader('dataset/',  r'tst.feature.conll', concepts)\n",
    "\n",
    "# training hmm on training data\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = hmm_model.train(trn_data.tagged_sents())\n",
    "\n",
    "# getting references\n",
    "refs = [str(t[1]) for s in tst_data.tagged_sents() for t in s]\n",
    "\n",
    "# getting hypotheses\n",
    "hyps = [str(t[1]) for s in tst_data.sents() for t in hmm_tagger.tag(s)]\n",
    "\n",
    "# print scores\n",
    "special = [\"''\", ':', None]\n",
    "tags = [c for c in concepts if c not in special]\n",
    "\n",
    "accuracy = hmm_tagger.evaluate(tst_data.tagged_sents())\n",
    "print(\"Accuracy: {:6.3f}\".format(accuracy*100))\n",
    "\n",
    "f1 = metrics.f1_score(refs, hyps, average='weighted', labels=np.unique(tags), zero_division=0)\n",
    "print(\"F1 score: {:6.3f}\".format(f1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-Tags NgramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff: 0\tNgram: 1\tAcc:  89.45\tF1 :  90.62\n",
      "Cutoff: 0\tNgram: 2\tAcc:  70.37\tF1 :  80.19\n",
      "Cutoff: 0\tNgram: 3\tAcc:  58.07\tF1 :  71.00\n",
      "\n",
      "\n",
      "Cutoff: 1\tNgram: 1\tAcc:  87.42\tF1 :  89.53\n",
      "Cutoff: 1\tNgram: 2\tAcc:  58.35\tF1 :  71.03\n",
      "Cutoff: 1\tNgram: 3\tAcc:  49.05\tF1 :  62.67\n",
      "\n",
      "\n",
      "\t\tCombined:\tAcc:  93.31\tF1 :  93.08\n"
     ]
    }
   ],
   "source": [
    "ngram = [1,2,3]\n",
    "cutoff = [0,1]\n",
    "for co in cutoff:\n",
    "    for ngo in ngram:\n",
    "        nm = NgramTagger(ngo, trn_data.tagged_sents(), cutoff=co)\n",
    "        acc = nm.evaluate(tst_data.tagged_sents())\n",
    "        \n",
    "        refs = [str(t[1]) for s in tst_data.tagged_sents() for t in s] \n",
    "        hyps = [str(t[1]) if t[1] is not None else str(t[1]) for s in tst_data.sents() for t in nm.tag(s)]\n",
    "        \n",
    "        f1 = metrics.f1_score(refs, hyps, average=\"weighted\", labels=np.unique(tags), zero_division=0)\n",
    "\n",
    "        print(\"Cutoff: {}\\tNgram: {}\\tAcc: {:6.2f}\\tF1 : {:6.2f}\".format(co,ngo,acc*100,f1*100))  \n",
    "    print(\"\\n\")\n",
    "\n",
    "backoff = DefaultTagger('NN')\n",
    "tag = backoff_tagger(trn_data.tagged_sents(),[UnigramTagger, BigramTagger, TrigramTagger],backoff=backoff)\n",
    "acc = tag.evaluate(tst_data.tagged_sents())\n",
    "refs = [str(t[1]) for s in tst_data.tagged_sents() for t in s]\n",
    "hyps = [str(t[1]) if t[1] is not None else str(t[1]) for s in tst_data.sents() for t in tag.tag(s)]\n",
    "f1 = metrics.f1_score(refs, hyps, average=\"weighted\", labels=np.unique(tags), zero_division=0)\n",
    "\n",
    "print(\"\\t\\tCombined:\\tAcc: {:6.2f}\\tF1 : {:6.2f}\".format(acc*100,f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
